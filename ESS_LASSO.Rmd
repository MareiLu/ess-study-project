---
title: "Project Study - LASSO variable selection"
author: "Helena Zappe & Marei Göbelbecker"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r install-packages, eval=FALSE, include=FALSE}

# Download Packages

install.packages(c(
  "tidyverse",  
  "skimr",      
  "psych",      
  "rmarkdown",  
  "knitr",      
  "tinytex",    
  "labelled",
  "here"
  ))
tinytex::install_tinytex()  
```

```{r load_packages, include=FALSE}

# Load Packages

library(haven)
library(tidyverse)   
library(glmnet)
library(skimr)       
library(psych)       
library(rmarkdown)   
library(knitr)       
library(tinytex)     
library(labelled)
library(dplyr)
library(here)
library(tidyr)
library(purrr)

```

```{r raw_data, include=FALSE}

# 1. Lade Daten und entferne nicht benötigte Variablen
dt_all <- read_sav(here("data", "ESS_prepared.sav")) %>%
  select(-c(trstun))
```


*`"What shapes trust in the European Union in Poland, Germany, and Slovenia? A comparative analysis of political attitudes, ideology, and socio-demographics."`*

# LASSO to find best variables

```{r}
# Step 1: Preparing Data for Lasso
# using the gilmnet package
  

# filter variables with too many NAs:
dt_filtered <- dt_all %>%
  drop_na(trstep) %>% # keep only valid observations in y
  select(where(~ !all(is.na(.)))) %>% # 100% NAs
  select(where(~ mean(is.na(.)) <= 0.8)) %>% # keep variables with less than 80% NAs
  select(-c(feethngr, loylead, lrnobed, donprty, volunfp, pbldmna, implvdm)) %>% # these variables are not relevant enough for imputation. Also households net income has 79% NAs and is therefore not meaningful. 
  select(-c(idno, proddate, bctprd, wrkorg, stratum, edition)) # Other variables that are not relevant for interpretation

# checking for missing variables in percent
dt_filtered %>%
  summarise(across(everything(), ~ mean(is.na(.)) * 100)) %>%
  pivot_longer(everything(), names_to = "variable", values_to = "missing_pct") %>%
  filter(missing_pct > 20) %>%
  arrange(missing_pct)

# accounting for differences in country size and selection
dt_filtered <- dt_filtered %>% drop_na(anweight) # dropping all obs where anweight is NA
weights <- dt_filtered$anweight # establishing the weighting

# defining the response Variable
y <- as.numeric(as_factor(dt_filtered$trstep))
table(y, useNA = "always")

# define the predictor variable matrix
x_data <- dt_filtered %>%
  select(-c(trstep, anweight, pspwght, dweight, pweight, prob)) %>%
  zap_labels() %>%  # delete all labels
  mutate(across(where(is.factor), ~ as.numeric(as.character(.)))) # define all factor variables also as numeric

x_data[is.na(x_data)] <- 0  # replaces all NA in x_data

# define the matrix of response variables
x <- data.matrix(x_data)

# LASSO with Crossvalidation and weighting
lasso_cv <- cv.glmnet(x, y, alpha = 1, weights = weights)

```

## Fitting the LASSO model:

The bigger lambda, the more coefficients will be set to 0 for variable selection.

**best_lambda** (lambda.min) gives the most precise model:

```{r}
# Find best Lambda
best_lambda <- lasso_cv$lambda.min

# Final LASSO Model
lasso_model <- glmnet(x, y, alpha = 1, lambda = best_lambda)

# Lambda Precise: 0.00127

# Extracting the Coefficients
coef_lasso <- coef(lasso_model)

selected_vars <- rownames(coef_lasso)[which(coef_lasso[, 1] != 0)]
selected_vars <- selected_vars[selected_vars != "(Intercept)"]

print(selected_vars)

```

**lambda.1se** gives the more robust, more economic (sparsamere) model: (Oft besser generalisierbar)

```{r}
lambda_1se <- lasso_cv$lambda.1se

model_robust <- glmnet(x, y, alpha = 1, lambda = lasso_cv$lambda.1se)

# Lambda Robust: 0.01308

# Extracting the Coefficients
coef_robust <- coef(model_robust)

vars_robust <- rownames(coef_robust)[which(coef_robust[, 1] != 0)]
vars_robust <- vars_robust[vars_robust != "(Intercept)"]

print(vars_robust)

```

Chatty: "In der LASSO-Analyse wurde mit lambda.min = 0.00140 das Modell mit der höchsten Vorhersagegenauigkeit identifiziert. Dieses Modell verwendet relativ viele Prädiktoren.

Alternativ wurde mit lambda.1se = 0.01575 ein robusteres Modell berechnet, das sparsamer ist und potenziell besser generalisiert. Hier bleiben nur die wichtigsten Prädiktoren erhalten.

Je nach Forschungsziel (Präzision vs. Interpretierbarkeit) kann eines der beiden Modelle bevorzugt werden."

## Model Specific Coefficients:

```{r}

# Vergleich
setdiff(selected_vars, vars_robust)  # Variablen nur im präzisen Modell
setdiff(vars_robust, selected_vars)  # Variablen nur im sparsamen Modell

```

```{r}
plot(lasso_cv)
abline(v = log(lasso_cv$lambda.min), col = "blue", lty = 3)
abline(v = log(lasso_cv$lambda.1se), col = "red", lty = 3)
abline(v = log(lambda_max), col = "green", lty = 3)
legend("topright", legend = c("lambda.min", "lambda.1se", "lambda_max"),
       col = c("blue", "red", "green"), lty = 3)
title("Cross-Validation: MSE über Lambda")
```

Chatty: - lambda.min (links) ergibt den niedrigsten mittleren Fehler – aber nutzt mehr Prädiktoren - lambda.1se (weiter rechts) hat leicht höheren Fehler, ist aber viel sparsamer Der Plot zeigt schön, dass der Fehler in einem sehr flachen Bereich minimal ist → daher kannst du das robustere, sparsamere Modell (lambda.1se) gut vertreten.

-\> Wir verwenden also die Variablen vom 2. Modell oder grenzen es weiter manuell ein.

### Neues Datenset lambda_max

```{r}
# dt_lasso_selected <- dt_filtered %>% select(all_of(vars_robust), trstep)
# insg. 76 variables
```

## Alternative: Interpretierbareres Modell mit max x Variablen

```{r}

lasso_model_max <- glmnet(x, y, alpha = 1, dfmax = 19)

# Extracting the Coefficients
coef_lasso <- coef(lasso_model_max)

# Anzahl ≠ 0-Koeffizienten je Lambda (inkl. Intercept)
nonzero <- lasso_model_max$df
lambda_index <- which(nonzero == 19)[1]  # x Prädiktoren + 1 Intercept

# Wenn gefunden:
if (!is.na(lambda_index)) {
  lambda_max <- lasso_model_max$lambda[lambda_index]
  coef_max <- coef(lasso_model_max, s = lambda_max)
  vars_max <- rownames(coef_max)[which(coef_max[, 1] != 0)]
  vars_max <- vars_max[vars_max != "(Intercept)"]
  print(vars_max)
} else {
  message("Kein Lambda mit genau x Prädiktoren gefunden.")
}


```

### Cutoff ausprobieren:

Chatty empfiehlt 10-30 Variablen

(Je geringer lambda, desto genauer passt das Model.) 

- Lambda 10 vars = not found 
- Lambda 15 vars = 0.1245 
- Lambda 16 vars = 0.1134
- Lambda 17 vars = not found
- Lambda 18 vars = 0.0858
- Lambda 19 vars = not found
- Lambda 20 vars = not found 
- Lambda 25 vars = 0.0591 
- Lambda 30 vars = not found 

Ich glaube 15 oder max. 20 sollten genug sein, sonst ist es zu viel.

**ACHTUNG!** Noch checken:

-   euftf & atcherp & trstplt -\> collinearity?
-   stfedu checken aussagekraft
-   possible indices: immigration & trust

### Neues Subset aus lambda_max 

```{r}
dt_lasso_selected <- dt_filtered %>% select(all_of(vars_max), trstep)
# insg. 20 variables

```

### Neues Subset mit originaler Codierung und Labels, aber nur lambda_max variablen

```{r}
#1. Neues Datenset mit Originalstruktur erstellen
  dt_lasso_labeled <- dt_all %>%
    select(all_of(vars_max), trstep)  
  
#2. Nur gültige Fälle von trstep behalten 
  dt_lasso_labeled <- dt_lasso_labeled %>%
    drop_na(trstep)  

#3. Optional: Labels prüfen
  library(labelled)
  look_for(dt_lasso_labeled)
```

### Speichern des Datensets in Dateien

```{r}
write_sav(dt_lasso_labeled, here("data", "ESS_lasso.sav"))
```


