---
title: 'Data Science Week 11: Text analysis'
author: "Tom Zimmermann"
output:
  html_document:
    toc: yes
    number_sections: yes
  pdf_document:
    toc: yes
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(tidyverse)
```


# "Just because you're paranoid, does not mean they're not out to get you."

The dataset `W11_Conspiracies.csv` contains articles from various websites active in the conspiracy theory space.^[Data are a pre-formatted subset of the data harvested in [LOCO: The 88-million-word language of conspiracy corpus](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8545361/)]
The columns are `website` (the website publishing the article), `title` (the title of the article), `text` (the text of the article), and `articleID` (a unique identifier for each article).


To familiarize yourself with text analysis it's helpful to start with some descriptive modeling. A useful tool for various tasks here is the `tidytext` package that is accompanied by the textbook *Text Mining with R* [available online here](https://www.tidytextmining.com/index.html). 

The lecture notes provide some to get you started using `tidytext` functions. 

1. Load the data.

```{r}
# Insert code here

dt = read_csv("/Users/test/Documents/Studium/Semester 3/Data Science/Data/raw-data/W11_Conspiracies.csv")

# End
```


2. Transform the data into tidytext format as discussed in the lecture notes.

```{r}
# Insert code here

library(tidytext)

dt_tidy = dt %>% 
  group_by(articleID) %>% 
  tidytext::unnest_tokens(output = term, 
                          input = text,
                          token = 'words',
                          to_lower = TRUE,
                          strip_punct = TRUE) %>% 
  ungroup()

# End
```


3. How often does the word "Covid" show up over all articles? What are the 20 most frequent terms across all articles?

```{r}
# Insert code here

# Count occurence of Covid
dt_tidy %>% 
  count(term) %>% 
  filter(term == "covid")

# Make list of most frequent terms
dt_tidy %>% 
  count(term, sort = TRUE)

# End
```


4. A lot of stopwords in that list! Remove them using a dictionary of stop words. (Hint: The `tidytext` package contains a list of stopwords (check `data(stop_words)`), see chapter 1.3 in the book for how to use that list to remove stop words from your data.). What are the 20 most frequent words after cleaning the articles for stop words? *Work with the data after removing stopwords for the remaining exercises.*


```{r}
# Insert code here

# Load dictionary of stopwords
stopwords <- tidytext::stop_words
stopwords

# Anti-join
dt_tidy <- dt_tidy %>% 
  anti_join(stopwords, by = c("term" = "word"))

# Count most frequent words after removing stopwords
dt_tidy %>% 
  count(term, sort = TRUE)

# End
```



5. For effect, represent the 100 most frequent terms via a _word cloud_. (Hint: If you are using `tidytext`, see chapter 2.5 of [Text mining with R](https://www.tidytextmining.com/))

```{r}
# Insert code here
library(wordcloud)

dt_tidy %>% 
  count(...) %>% 
  with(wordcloud(..., ..., max.words = 100))


# End
```


6. Next, figure out which articles have the most negative sentiment. To do that, use the [NRC lexicon](http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm) that can easily be imported via `tidytext`. (You might need to install the `textdata` package to make the code in the lecture notes work.)


What are the 5 articles with the highest number of negative words?

```{r}
# Insert code here

library(textdata)

# Load sentiment dictionary
sentiments <- get_sentiments("nrc") %>% 
  filter(sentiment %in% c("positive", "negative"))

sentiments
# Count number of negative words per article
dt_tidy %>% 
  # Join sentiment to tidy text table
  left_join(sentiments, by=c("term"="word")) %>% 
  filter(sentiment == 'negative') %>% 
  # Compute number of negative words by article
  group_by(articleID) %>% 
  summarise(NumNegWords = n()) %>% 
  ungroup() %>% 
  arrange(desc(NumNegWords)) 

# End
```



7. (Optional) Just counting the number of negative words to determine the most negative articles as in part 6 biases the ranking towards longer articles. Rank by the *share of negative words relative to the sum of positive and negative words* in each article instead.


```{r}
# Insert code here

dt_tidy %>% 
  # Join sentiment to tidy text table
  left_join(sentiments, by=c("term"="word")) %>% 
  group_by(articleID, sentiment) %>%
  summarise(nTerms = n()) %>%
  # Compute number of negative words by article
  group_by(articleID) %>% 
  mutate(Share = nTerms/sum(nTerms)) %>% 
  ungroup() %>% 
  filter(sentiment == "negative") %>%
  arrange(desc(Share)) 

# End
```


8. More generally, we might be interested in the different emotions that are expressed in conspiracy articles. The [NRC lexikon](http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm) also associates words with one of eight emotions (anger, anticipation, disgust, fear, joy, sadness, surprise, trust). Count the number of times that each emotion is expressed *across all articles*. Hint: You get the emotions part of the lexicon via

```{r}
# Load emotions dictionary
nrc_emotions = read_csv("/Users/test/Documents/Studium/Semester 3/Data Science/Data/raw-data/NRC.csv") %>%
  filter(sentiment != "negative" & sentiment !="positive")
```


```{r}
# Insert code here
dt_tidy  %>% 
  # Join emotions
  left_join(nrc_emotions, by=c("term"="word")) %>% 
  filter(!is.na(sentiment)) %>% 
  # Count terms by emotion (sentiment)
  group_by(articleID, sentiment) %>% 
  summarise(NumWordsByEmotion = n()) %>% 
  ungroup() %>% 
  arrange(desc(NumWordsByEmotion))

# End
```